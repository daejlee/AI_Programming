{"cells":[{"cell_type":"markdown","metadata":{},"source":["\n","# 이론\n","머신러닝 기술들은 아래 세 가지 작업 중 하나에 해당된다.\n","- 지도 학습: 레이블이 존재. 분류, 회귀.\n","- 비지도 학습: 레이블이 없음. 주로 데이터를 그룹화.\n","- 강화 학습: 보상 시스템\n","#### 선형 회귀\n","데이터와 레이블의 관계를 선형으로 가정하고 문제 해결.   \n","가설: 머신러닝에서 데이터와 레이블 사이의 관계에 대해 가정한 사실들.   \n","선형 회귀에서는 **y=Wx+b**와 같은 가설을 수립한다.\n","- W: 가중치(weight)\n","- b: 편향(bias)   \n","\n","**선형 회귀의 목표는, 최적의 W, b를 찾아내는 것이다.**   \n","MSE, Mean Squred Error는 가장 간단하고 널리 쓰이는 손실 함수이다.   \n","**경사하강법**이라는 최적화 기법을 주로 활용한다. 이는 W, b 값을 조금씩 변경하며 손실이 더 적은 값을 찾아가는 방식이다. 변경 방향은 미분을 통해 알 수 있으며, **학습률**을 통해 한번에 변경하는 값의 양을 결정한다."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":607,"status":"ok","timestamp":1695187083818,"user":{"displayName":"DAEJIN LEE","userId":"03094455903435104023"},"user_tz":-540},"id":"XobLkENR9hvV","outputId":"1c474e37-2d1d-418c-aea3-2f95de87e4c9"},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as f\n","import torch.optim as optim\n","\n","x_train = torch.FloatTensor([[1], [2], [3]]).view(-1, 1)\n","y_train = torch.FloatTensor([[2], [4], [6]]).view(-1, 1)"]},{"cell_type":"markdown","metadata":{},"source":["데이터셋을 생성한다.   \n","토치에서 데이터셋을 나타내는 텐서는, 첫번째 차원을 데이터의 순번을 나타내기 위한 차원으로 사용한다."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["W = torch.zeros(1, requires_grad=True)\n","b = torch.zeros(1, requires_grad=True)\n","#가설 수립\n","hypo = x_train * W + b\n","print(hypo)"]},{"cell_type":"markdown","metadata":{},"source":["가중치와 편향을 생성한다.\n","- requires_grad=True 옵션으로 autograd를 위한 변수임을 나타낸다.\n","  - False로 입력하면 손수 미분값을 다뤄야한다."]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6,"status":"ok","timestamp":1695187120844,"user":{"displayName":"DAEJIN LEE","userId":"03094455903435104023"},"user_tz":-540},"id":"ktTA2RlD99i9","outputId":"4a035a97-ad05-483e-f2db-9c5f21d9c0c8"},"outputs":[{"name":"stdout","output_type":"stream","text":["tensor(18.6667, grad_fn=<MeanBackward0>)\n"]}],"source":["#비용함수 선언\n","cost = torch.mean((hypo - y_train) ** 2)\n","print (cost)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":570,"status":"ok","timestamp":1695187670747,"user":{"displayName":"DAEJIN LEE","userId":"03094455903435104023"},"user_tz":-540},"id":"qBI1hiDSBq_O","outputId":"be575ba1-6f18-4382-e2f6-c274e66bb718"},"outputs":[],"source":["W = torch.zeros(1, requires_grad=True)\n","b = torch.zeros(1, requires_grad=True)\n","optimizer = optim.SGD([W, b], lr=0.01)\n","nb_epochs = 1000\n","\n","for e in range(nb_epochs + 1):\n","  hypo = x_train * W + b\n","  cost = torch.mean((hypo - y_train) ** 2)\n","\n","  optimizer.zero_grad()\n","  cost.backward()\n","  optimizer.step()\n","\n","  if e % 100 == 0:\n","    print('Epoch {:4d}/{} W: {:.3f} b: {:.3f} Cost: {:.6f}'.format(e, nb_epochs, W.item(), b.item(), cost.item()))"]},{"cell_type":"markdown","metadata":{},"source":["경사하강법과 학습 진행 코드의 구현이다.\n","- 경사하강법은 optim.SGD를 활용한다.\n","- 학습률은 0.01이다.   \n","\n","주어진 손실함수에 대한 미분값을 구하는 과정은 .backword()로 이루어지며 step()을 통해 W, b값이 조금씩 바뀌게 된다."]}],"metadata":{"colab":{"authorship_tag":"ABX9TyNZ3KP42ia66mZ5opdQl5Oq","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.1"}},"nbformat":4,"nbformat_minor":0}
